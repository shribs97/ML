{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979caa88",
   "metadata": {},
   "source": [
    "NAME:-SHRIDATTA SHEKHAR BHASME\n",
    "ROLL NO :- RBTL22CB072\n",
    "SUBJECT:- MACHINE LEARNING\n",
    "DATASET :- PENGUINS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f63ac",
   "metadata": {},
   "source": [
    "Aim:\n",
    "The aim of this study is to comprehensively explore and evaluate ensemble learning algorithms, specifically AdaBoost, Gradient Boosting, XGBoost, and CatBoost, for classification tasks. The goal is to understand the strengths and weaknesses of each algorithm, identify their optimal parameter configurations, and compare their performance on a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191d43f",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "Understand Ensemble Learning: Conduct a literature review to gain a thorough understanding of ensemble learning, its principles, and the advantages it offers in improving model performance.\n",
    "\n",
    "Algorithm Exploration:\n",
    "a. Implement and experiment with AdaBoost, Gradient Boosting, XGBoost, and CatBoost algorithms.\n",
    "b. Explore and tune hyperparameters to optimize the performance of each algorithm.\n",
    "c. Investigate the impact of ensemble size (number of base learners) on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8671d",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "The field of machine learning is rapidly evolving, with various ensemble techniques emerging as powerful tools for improving predictive performance. However, there is a lack of comprehensive understanding regarding the strengths, weaknesses, and optimal use cases of popular ensemble techniques such as AdaBoost, Gradient Boosting, XGBoost, and CatBoost. The absence of a thorough comparative analysis hinders practitioners and researchers in selecting the most suitable ensemble method for different types of datasets and applications. Therefore, there is a need for a detailed study to compare and contrast these ensemble techniques to guide practitioners in making informed choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7b903",
   "metadata": {},
   "source": [
    "Theory:\n",
    "Ensemble techniques are a class of machine learning methods that combine the predictions of multiple base models to achieve better overall performance than individual models. The selected ensemble techniques for this comparative analysis include AdaBoost, Gradient Boosting, XGBoost, and CatBoost, each known for its unique characteristics.\n",
    "\n",
    "AdaBoost (Adaptive Boosting):\n",
    "\n",
    "AdaBoost focuses on combining weak learners to create a strong learner.\n",
    "It assigns weights to misclassified instances to give more emphasis on the difficult-to-classify samples.\n",
    "The final prediction is a weighted sum of the weak learners.\n",
    "Gradient Boosting:\n",
    "\n",
    "Gradient Boosting builds a sequence of decision trees, where each tree corrects the errors of the previous one.\n",
    "It minimizes a loss function using gradient descent during the training process.\n",
    "Gradient Boosting is known for its flexibility and ability to handle various types of data.\n",
    "XGBoost (Extreme Gradient Boosting):\n",
    "\n",
    "XGBoost is an optimized version of Gradient Boosting, designed for speed and performance.\n",
    "It includes regularization terms to control overfitting and has efficient handling of missing data.\n",
    "XGBoost is widely used in data science competitions and has become a popular choice in many applications.\n",
    "CatBoost:\n",
    "\n",
    "CatBoost is a gradient boosting library that is particularly effective with categorical features.\n",
    "It employs a symmetric tree structure and utilizes ordered boosting to handle categorical variables naturally.\n",
    "CatBoost is known for its out-of-the-box support for categorical data and automatic handling of parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16419bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2939a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"penguins.csv\")\n",
    "data.head(5)\n",
    "\n",
    "# Assuming 'Loan_Status' is the target variable\n",
    "X = data.drop('species', axis=1)\n",
    "y = data['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3690e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder=preprocessing.LabelEncoder()\n",
    "data['species']=label_encoder.fit_transform(data['species'])\n",
    "data['island']=label_encoder.fit_transform(data['island'])\n",
    "data['sex']=label_encoder.fit_transform(data['sex'])\n",
    "data['bill_length_mm']=label_encoder.fit_transform(data['bill_length_mm'])\n",
    "data['bill_depth_mm']=label_encoder.fit_transform(data['bill_depth_mm'])\n",
    "data['flipper_length_mm']=label_encoder.fit_transform(data['flipper_length_mm'])\n",
    "data['body_mass_g']=label_encoder.fit_transform(data['body_mass_g'])\n",
    "\n",
    "X = data.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f61378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(data,random_state=42)\n",
    "x_train=train[train.columns[2:30]]\n",
    "y_train =train['species']\n",
    "x_test=test[test.columns[2:30]]\n",
    "y_test =test['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da207964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.8023\n"
     ]
    }
   ],
   "source": [
    "ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_model.fit(x_train, y_train)\n",
    "ada_predictions = ada_model.predict(x_test)\n",
    "ada_accuracy = accuracy_score(y_test, ada_predictions)\n",
    "print(f'AdaBoost Accuracy: {ada_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a28a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "gb_model.fit(x_train, y_train)\n",
    "gb_predictions = gb_model.predict(x_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_predictions)\n",
    "print(f'Gradient Boosting Accuracy: {gb_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db809483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier\n",
    "xgb_model = XGBClassifier(n_estimators=50, random_state=42)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(x_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "print(f'XGBoost Accuracy: {xgb_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c531c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# CatBoost Classifier\n",
    "cat_model = CatBoostClassifier(iterations=50, random_state=42, verbose=False)\n",
    "cat_model.fit(x_train, y_train)\n",
    "cat_predictions = cat_model.predict(x_test)\n",
    "cat_accuracy = accuracy_score(y_test, cat_predictions)\n",
    "print(f'CatBoost Accuracy: {cat_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e93af",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "Handling Categorical Features:\n",
    "\n",
    "CatBoost excels in handling categorical features directly, eliminating the need for extensive preprocessing.\n",
    "XGBoost and Gradient Boosting require one-hot encoding or similar preprocessing for categorical features.\n",
    "Performance:\n",
    "\n",
    "The performance can vary based on the dataset and the specific problem at hand.\n",
    "XGBoost and CatBoost often provide competitive performance and are preferred in many real-world scenarios.\n",
    "Interpretability:\n",
    "\n",
    "AdaBoost and Gradient Boosting models are more interpretable compared to XGBoost and CatBoost, which are known for their black-box nature.\n",
    "Robustness:\n",
    "\n",
    "CatBoost is designed to be robust and handles noisy data well.\n",
    "Gradient Boosting is robust to outliers but may struggle with noisy data.\n",
    "AdaBoost can be sensitive to noisy data.\n",
    "Speed:\n",
    "\n",
    "XGBoost is known for its speed and scalability.\n",
    "CatBoost, while generally efficient, may have longer training times in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cb131",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Summarize the findings and draw conclusions based on the evaluation of AdaBoost, Gradient Boosting, XGBoost, and CatBoost. Highlight the following aspects:\n",
    "\n",
    "Performance: Identify the algorithm that performs best on the specific dataset and under different evaluation metrics.\n",
    "Robustness: Evaluate the robustness of each algorithm to variations in the dataset and the impact of outliers.\n",
    "Interpretability: Discuss the interpretability of the models and their ability to provide insights into feature importance.\n",
    "Computational Efficiency: Consider the computational efficiency of each algorithm, especially in terms of training time and prediction speed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
